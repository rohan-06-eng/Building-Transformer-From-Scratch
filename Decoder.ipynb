{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A single layer of the Decoder. Usually there are multiple layers stacked on top of each other.\n",
    "    \n",
    "    Methods:\n",
    "        call: Performs the forward pass of the layer.\n",
    "\n",
    "    Attributes:\n",
    "        causal_self_attention (CausalSelfAttention): The causal self-attention layer.\n",
    "        cross_attention (CrossAttention): The cross-attention layer.\n",
    "        ffn (FeedForward): The feed-forward layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, dff: int, dropout_rate: float=0.1):\n",
    "        \"\"\"\n",
    "        Constructor of the DecoderLayer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimensionality of the model.\n",
    "            num_heads (int): The number of heads in the multi-head attention layer.\n",
    "            dff (int): The dimensionality of the feed-forward layer.\n",
    "            dropout_rate (float): The dropout rate. \n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x: tf.Tensor, context: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        The call function that performs the forward pass of the layer.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): The input sequence of shape (batch_size, seq_length, d_model). x is usually the output of the previous decoder layer.\n",
    "            context (tf.Tensor): The context sequence of shape (batch_size, seq_length, d_model). Context is usually the output of the encoder.\n",
    "        \"\"\"\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nx represents the count of how many EncoderLayers we use in the whole Encoder. Let's implement the Encoder layer in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A custom TensorFlow layer that implements the Decoder. This layer is mostly used in the Transformer models\n",
    "    for natural language processing tasks, such as machine translation, text summarization or text classification.\n",
    "\n",
    "    Methods:\n",
    "        call: Performs the forward pass of the layer.\n",
    "\n",
    "    Attributes:\n",
    "        d_model (int): The dimensionality of the model.\n",
    "        num_layers (int): The number of layers in the decoder.\n",
    "        pos_embedding (PositionalEmbedding): The positional embedding layer.\n",
    "        dec_layers (list): The list of decoder layers.\n",
    "        dropout (tf.keras.layers.Dropout): The dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, dff: int, vocab_size: int, dropout_rate: float=0.1):\n",
    "        \"\"\"\n",
    "        Constructor of the Decoder.\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): The number of layers in the decoder.\n",
    "            d_model (int): The dimensionality of the model.\n",
    "            num_heads (int): The number of heads in the multi-head attention layer.\n",
    "            dff (int): The dimensionality of the feed-forward layer.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "            dropout_rate (float): The dropout rate.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(\n",
    "                d_model=d_model, \n",
    "                num_heads=num_heads, \n",
    "                dff=dff, \n",
    "                dropout_rate=dropout_rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x: tf.Tensor, context: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        The call function that performs the forward pass of the layer.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): The input sequence of shape (batch_size, target_seq_len).\n",
    "            context (tf.Tensor): The context sequence of shape (batch_size, input_seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x  = self.dec_layers[i](x, context)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
