# Transformer From Scratch - Translation

This project demonstrates how to build a Transformer model from scratch for language translation tasks. The Transformer model, introduced in the paper "Attention is All You Need" by Vaswani et al., has become the foundation for many state-of-the-art NLP models.
The model is based on the Transformer (self-attention) architecture, an alternative to recurrent neural networks (RNNs). The Transformer model is more straightforward than traditional recurrent neural networks and has achieved state-of-the-art results on machine translation tasks.

This includes building various class and methods for building Transformer.
- Implementing positional embedding layer
- Implementing Attention layers
- Implementing Encoder and Decoder layers
- Building the Transformer model
- Preprocessing data for machine translation task
- Training and Evaluating the model
