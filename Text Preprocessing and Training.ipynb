{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-hi//opus-100-corpus/v1.0/supervised/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:00<00:03,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-hi//opus-100-corpus/v1.0/supervised/\n",
      "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-hi/opus.en-hi-dev.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:01<00:04,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved opus.en-hi-dev.en\n",
      "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-hi/opus.en-hi-dev.hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [00:03<00:05,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved opus.en-hi-dev.hi\n",
      "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-hi/opus.en-hi-test.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [00:04<00:03,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved opus.en-hi-test.en\n",
      "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-hi/opus.en-hi-test.hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:06<00:02,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved opus.en-hi-test.hi\n",
      "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-hi/opus.en-hi-train.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [01:10<00:22, 22.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved opus.en-hi-train.en\n",
      "Downloading https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-hi/opus.en-hi-train.hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [03:07<00:00, 26.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved opus.en-hi-train.hi\n",
      "All files have been downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to the directory containing the files to be downloaded\n",
    "language = \"en-hi\"\n",
    "url = f\"https://data.statmt.org/opus-100-corpus/v1.0/supervised/{language}/\"\n",
    "save_directory = f\"./Datasets/{language}\"\n",
    "\n",
    "# Create the save directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML response\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the anchor tags in the HTML\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# Extract the href attribute from each anchor tag\n",
    "file_links = [link['href'] for link in links if '.' in link['href']]\n",
    "\n",
    "# Download each file\n",
    "for file_link in tqdm(file_links):\n",
    "    file_url = url + file_link\n",
    "    save_path = os.path.join(save_directory, file_link)\n",
    "    \n",
    "    print(f\"Downloading {file_url}\")\n",
    "    \n",
    "    # Send a GET request for the file\n",
    "    file_response = requests.get(file_url)\n",
    "    if file_response.status_code == 404:\n",
    "        print(f\"Could not download {file_url}\")\n",
    "        continue\n",
    "    \n",
    "    # Save the file to the specified directory\n",
    "    with open(save_path, 'wb') as file:\n",
    "        file.write(file_response.content)\n",
    "    \n",
    "    print(f\"Saved {file_link}\")\n",
    "\n",
    "print(\"All files have been downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531604\n",
      "1986\n",
      "('अन्य, निज़ी उपयोग', 'ऊबड़ .', 'जीवनसाथी')\n",
      "('Other, Private Use', '[SCREAMING]', 'Spouse')\n"
     ]
    }
   ],
   "source": [
    "en_training_data_path = \"Datasets/en-hi/opus.en-hi-train.en\"\n",
    "en_validation_data_path = \"Datasets/en-hi/opus.en-hi-dev.en\"\n",
    "hi_training_data_path = \"Datasets/en-hi/opus.en-hi-train.hi\"\n",
    "hi_validation_data_path = \"Datasets/en-hi/opus.en-hi-dev.hi\"\n",
    "\n",
    "def read_filhi(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        en_train_dataset = f.read().split(\"\\n\")[:-1]\n",
    "    return en_train_dataset\n",
    "\n",
    "en_training_data = read_filhi(en_training_data_path)\n",
    "en_validation_data = read_filhi(en_validation_data_path)\n",
    "hi_training_data = read_filhi(hi_training_data_path)\n",
    "hi_validation_data = read_filhi(hi_validation_data_path)\n",
    "\n",
    "max_lenght = 500\n",
    "train_dataset = [[hi_sentence, en_sentence] for hi_sentence, en_sentence in zip(hi_training_data, en_training_data) if len(hi_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "val_dataset = [[hi_sentence, en_sentence] for hi_sentence, en_sentence in zip(hi_validation_data, en_validation_data) if len(hi_sentence) <= max_lenght and len(en_sentence) <= max_lenght]\n",
    "hi_training_data, en_training_data = zip(*train_dataset)\n",
    "hi_validation_data, en_validation_data = zip(*val_dataset)\n",
    "\n",
    "print(len(hi_training_data))\n",
    "print(len(hi_validation_data))\n",
    "print(hi_training_data[:3])\n",
    "print(en_training_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import typing\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomTokenizer:\n",
    "    \"\"\" Custom Tokenizer class to tokenize and detokenize text data into sequences of integers\n",
    "\n",
    "    Args:\n",
    "        split (str, optional): Split token to use when tokenizing text. Defaults to \" \".\n",
    "        char_level (bool, optional): Whether to tokenize at character level. Defaults to False.\n",
    "        lower (bool, optional): Whether to convert text to lowercase. Defaults to True.\n",
    "        start_token (str, optional): Start token to use when tokenizing text. Defaults to \"<start>\".\n",
    "        end_token (str, optional): End token to use when tokenizing text. Defaults to \"<eos>\".\n",
    "        filters (list, optional): List of characters to filter out. Defaults to \n",
    "            ['!', \"'\", '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', \n",
    "            '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n'].\n",
    "        filter_nums (bool, optional): Whether to filter out numbers. Defaults to True.\n",
    "        start (int, optional): Index to start tokenizing from. Defaults to 1.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            split: str=\" \", \n",
    "            char_level: bool=False,\n",
    "            lower: bool=True, \n",
    "            start_token: str=\"<start>\", \n",
    "            end_token: str=\"<eos>\",\n",
    "            filters: list = ['!', \"'\", '\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n'],\n",
    "            filter_nums: bool = True,\n",
    "            start: int=1,\n",
    "        ) -> None:\n",
    "        self.split = split\n",
    "        self.char_level = char_level\n",
    "        self.lower = lower\n",
    "        self.index_word = {}\n",
    "        self.word_index = {}\n",
    "        self.max_length = 0\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.filters = filters\n",
    "        self.filter_nums = filter_nums\n",
    "        self.start = start\n",
    "\n",
    "    @property\n",
    "    def start_token_index(self):\n",
    "        return self.word_index[self.start_token]\n",
    "    \n",
    "    @property\n",
    "    def end_token_index(self):\n",
    "        return self.word_index[self.end_token]\n",
    "\n",
    "    def sort(self):\n",
    "        \"\"\" Sorts the word_index and index_word dictionaries\"\"\"\n",
    "        self.index_word = dict(enumerate(dict(sorted(self.word_index.items())), start=self.start))\n",
    "        self.word_index = {v: k for k, v in self.index_word.items()}\n",
    "\n",
    "    def split_line(self, line: str):\n",
    "        \"\"\" Splits a line of text into tokens\n",
    "\n",
    "        Args:\n",
    "            line (str): Line of text to split\n",
    "\n",
    "        Returns:\n",
    "            list: List of string tokens\n",
    "        \"\"\"\n",
    "        line = line.lower() if self.lower else line\n",
    "\n",
    "        if self.char_level:\n",
    "            return [char for char in line]\n",
    "\n",
    "        # split line with split token and check for filters\n",
    "        line_tokens = line.split(self.split)\n",
    "\n",
    "        new_tokens = []\n",
    "        for index, token in enumerate(line_tokens):\n",
    "            filtered_tokens = ['']\n",
    "            for c_index, char in enumerate(token):\n",
    "                if char in self.filters or (self.filter_nums and char.isdigit()):\n",
    "                    filtered_tokens += [char, ''] if c_index != len(token) -1 else [char]\n",
    "                else:\n",
    "                    filtered_tokens[-1] += char\n",
    "\n",
    "            new_tokens += filtered_tokens\n",
    "            if index != len(line_tokens) -1:\n",
    "                new_tokens += [self.split]\n",
    "\n",
    "        new_tokens = [token for token in new_tokens if token != '']\n",
    "\n",
    "        return new_tokens\n",
    "\n",
    "    def fit_on_texts(self, lines: typing.List[str]):\n",
    "        \"\"\" Fits the tokenizer on a list of lines of text\n",
    "        This function will update the word_index and index_word dictionaries and set the max_length attribute\n",
    "\n",
    "        Args:\n",
    "            lines (typing.List[str]): List of lines of text to fit the tokenizer on\n",
    "        \"\"\"\n",
    "        self.word_index = {key: value for value, key in enumerate([self.start_token, self.end_token, self.split] + self.filters)}\n",
    "        \n",
    "        for line in tqdm(lines, desc=\"Fitting tokenizer\"):\n",
    "            line_tokens = self.split_line(line)\n",
    "            self.max_length = max(self.max_length, len(line_tokens) +2) # +2 for start and end tokens\n",
    "\n",
    "            for token in line_tokens:\n",
    "                if token not in self.word_index:\n",
    "                    self.word_index[token] = len(self.word_index)\n",
    "\n",
    "        self.sort()\n",
    "\n",
    "    def update(self, lines: typing.List[str]):\n",
    "        \"\"\" Updates the tokenizer with new lines of text\n",
    "        This function will update the word_index and index_word dictionaries and set the max_length attribute\n",
    "\n",
    "        Args:\n",
    "            lines (typing.List[str]): List of lines of text to update the tokenizer with\n",
    "        \"\"\"\n",
    "        new_tokens = 0\n",
    "        for line in tqdm(lines, desc=\"Updating tokenizer\"):\n",
    "            line_tokens = self.split_line(line)\n",
    "            self.max_length = max(self.max_length, len(line_tokens) +2) # +2 for start and end tokens\n",
    "            for token in line_tokens:\n",
    "                if token not in self.word_index:\n",
    "                    self.word_index[token] = len(self.word_index)\n",
    "                    new_tokens += 1\n",
    "\n",
    "        self.sort()\n",
    "        print(f\"Added {new_tokens} new tokens\")\n",
    "\n",
    "    def detokenize(self, sequences: typing.List[int], remove_start_end: bool=True):\n",
    "        \"\"\" Converts a list of sequences of tokens back into text\n",
    "\n",
    "        Args:\n",
    "            sequences (typing.list[int]): List of sequences of tokens to convert back into text\n",
    "            remove_start_end (bool, optional): Whether to remove the start and end tokens. Defaults to True.\n",
    "        \n",
    "        Returns:\n",
    "            typing.List[str]: List of strings of the converted sequences\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        for sequence in sequences:\n",
    "            line = \"\"\n",
    "            for token in sequence:\n",
    "                if token == 0:\n",
    "                    break\n",
    "                if remove_start_end and (token == self.start_token_index or token == self.end_token_index):\n",
    "                    continue\n",
    "\n",
    "                line += self.index_word[token]\n",
    "\n",
    "            lines.append(line)\n",
    "\n",
    "        return lines\n",
    "\n",
    "    def texts_to_sequences(self, lines: typing.List[str], include_start_end: bool=True):\n",
    "        \"\"\" Converts a list of lines of text into a list of sequences of tokens\n",
    "        \n",
    "        Args:\n",
    "            lines (typing.list[str]): List of lines of text to convert into tokenized sequences\n",
    "            include_start_end (bool, optional): Whether to include the start and end tokens. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            typing.List[typing.List[int]]: List of sequences of tokens\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        for line in lines:\n",
    "            line_tokens = self.split_line(line)\n",
    "            sequence = [self.word_index[word] for word in line_tokens if word in self.word_index]\n",
    "            if include_start_end:\n",
    "                sequence = [self.word_index[self.start_token]] + sequence + [self.word_index[self.end_token]]\n",
    "\n",
    "            sequences.append(sequence)\n",
    "\n",
    "        return sequences\n",
    "    \n",
    "    def save(self, path: str, type: str=\"json\"):\n",
    "        \"\"\" Saves the tokenizer to a file\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to save the tokenizer to\n",
    "            type (str, optional): Type of file to save the tokenizer to. Defaults to \"json\".\n",
    "        \"\"\"\n",
    "        serialised_dict = self.dict()\n",
    "        if type == \"json\":\n",
    "            if os.path.dirname(path):\n",
    "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump(serialised_dict, f)\n",
    "\n",
    "    def dict(self):\n",
    "        \"\"\" Returns a dictionary of the tokenizer\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of the tokenizer\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"split\": self.split,\n",
    "            \"lower\": self.lower,\n",
    "            \"char_level\": self.char_level,\n",
    "            \"index_word\": self.index_word,\n",
    "            \"max_length\": self.max_length,\n",
    "            \"start_token\": self.start_token,\n",
    "            \"end_token\": self.end_token,\n",
    "            \"filters\": self.filters,\n",
    "            \"filter_nums\": self.filter_nums,\n",
    "            \"start\": self.start\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: typing.Union[str, dict], type: str=\"json\"):\n",
    "        \"\"\" Loads a tokenizer from a file\n",
    "\n",
    "        Args:\n",
    "            path (typing.Union[str, dict]): Path to load the tokenizer from or a dictionary of the tokenizer\n",
    "            type (str, optional): Type of file to load the tokenizer from. Defaults to \"json\".\n",
    "\n",
    "        Returns:\n",
    "            CustomTokenizer: Loaded tokenizer\n",
    "        \"\"\"\n",
    "        if isinstance(path, str):\n",
    "            if type == \"json\":\n",
    "                with open(path, \"r\") as f:\n",
    "                    load_dict = json.load(f)\n",
    "\n",
    "        elif isinstance(path, dict):\n",
    "            load_dict = path\n",
    "\n",
    "        tokenizer = CustomTokenizer()\n",
    "        tokenizer.split = load_dict[\"split\"]\n",
    "        tokenizer.lower = load_dict[\"lower\"]\n",
    "        tokenizer.char_level = load_dict[\"char_level\"]\n",
    "        tokenizer.index_word = {int(k): v for k, v in load_dict[\"index_word\"].items()}\n",
    "        tokenizer.max_length = load_dict[\"max_length\"]\n",
    "        tokenizer.start_token = load_dict[\"start_token\"]\n",
    "        tokenizer.end_token = load_dict[\"end_token\"]\n",
    "        tokenizer.filters = load_dict[\"filters\"]\n",
    "        tokenizer.filter_nums = bool(load_dict[\"filter_nums\"])\n",
    "        tokenizer.start = load_dict[\"start\"]\n",
    "        tokenizer.word_index = {v: int(k) for k, v in tokenizer.index_word.items()}\n",
    "\n",
    "        return tokenizer\n",
    "    \n",
    "    @property\n",
    "    def lenght(self):\n",
    "        return len(self.index_word)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer: 100%|██████████| 531604/531604 [00:07<00:00, 71208.45it/s]\n",
      "Fitting tokenizer: 100%|██████████| 531604/531604 [00:04<00:00, 112074.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare Hindi tokenizer, this is the input language\n",
    "tokenizer = CustomTokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(hi_training_data)\n",
    "tokenizer.save(f\"./Token-Detoken/tokenizer.json\")\n",
    "\n",
    "# Prepare English tokenizer, this is the output language\n",
    "detokenizer = CustomTokenizer(char_level=True)\n",
    "detokenizer.fit_on_texts(en_training_data)\n",
    "detokenizer.save(f\"./Token-Detoken/detokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 51, 48, 55, 55, 58, 3, 66, 58, 61, 55, 47, 15, 3, 51, 58, 66, 3, 44, 61, 48, 3, 68, 58, 64, 36, 32]\n",
      "['<start>hello world, how are you?<eos>']\n",
      "['hello world, how are you?']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = detokenizer.texts_to_sequences([\"Hello world, how are you?\"])[0]\n",
    "print(tokenized_sentence)\n",
    "\n",
    "detokenized_sentence = detokenizer.detokenize([tokenized_sentence], remove_start_end=False)\n",
    "print(detokenized_sentence)\n",
    "\n",
    "detokenized_sentence = detokenizer.detokenize([tokenized_sentence])\n",
    "print(detokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_inputs(data_batch, label_batch):\n",
    "    encoder_input = np.zeros((len(data_batch), tokenizer.max_length)).astype(np.int64)\n",
    "    decoder_input = np.zeros((len(label_batch), detokenizer.max_length)).astype(np.int64)\n",
    "    decoder_output = np.zeros((len(label_batch), detokenizer.max_length)).astype(np.int64)\n",
    "\n",
    "    data_batch_tokens = tokenizer.texts_to_sequences(data_batch)\n",
    "    label_batch_tokens = detokenizer.texts_to_sequences(label_batch)\n",
    "\n",
    "    for index, (data, label) in enumerate(zip(data_batch_tokens, label_batch_tokens)):\n",
    "        encoder_input[index][:len(data)] = data\n",
    "        decoder_input[index][:len(label)-1] = label[:-1] # Drop the [END] tokens\n",
    "        decoder_output[index][:len(label)-1] = label[1:] # Drop the [START] tokens\n",
    "\n",
    "    return (encoder_input, decoder_input), decoder_output\n",
    "\n",
    "train_dataProvider = DataProvider(\n",
    "    train_dataset, \n",
    "    batch_size=4, \n",
    "    batch_postprocessors=[preprocess_inputs],\n",
    "    use_cache=True\n",
    "    )\n",
    "\n",
    "val_dataProvider = DataProvider(\n",
    "    val_dataset, \n",
    "    batch_size=4, \n",
    "    batch_postprocessors=[preprocess_inputs],\n",
    "    use_cache=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['अन्य, निज़ी उपयोग', 'ऊबड़ .', 'जीवनसाथी', '- तुम एक कमांडर कभी नहीं होगा!']\n",
      "['<start>other, private use', '<start>[screaming]', '<start>spouse', '<start>i will never salute you!']\n",
      "['other, private use<eos>', '[screaming]<eos>', 'spouse<eos>', 'i will never salute you!<eos>']\n"
     ]
    }
   ],
   "source": [
    "for data_batch in train_dataProvider:\n",
    "    (encoder_inputs, decoder_inputs), decoder_outputs = data_batch\n",
    "\n",
    "    encoder_inputs_str = tokenizer.detokenize(encoder_inputs)\n",
    "    decoder_inputs_str = detokenizer.detokenize(decoder_inputs, remove_start_end=False)\n",
    "    decoder_outputs_str = detokenizer.detokenize(decoder_outputs, remove_start_end=False)\n",
    "    print(encoder_inputs_str)\n",
    "    print(decoder_inputs_str)\n",
    "    print(decoder_outputs_str)\n",
    "    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
