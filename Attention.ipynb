{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Base class for all attention layers. It contains the common functionality of all attention layers.\n",
    "    This layer contains a MultiHeadAttention layer, a LayerNormalization layer and an Add layer.\n",
    "    It is used as a base class for the GlobalSelfAttention, CausalSelfAttention and CrossAttention layers.\n",
    "    And it is not intended to be used directly.\n",
    "\n",
    "    Methods:\n",
    "        call: Performs the forward pass of the layer.\n",
    "\n",
    "    Attributes:\n",
    "        mha (tf.keras.layers.MultiHeadAttention): The MultiHeadAttention layer.\n",
    "        layernorm (tf.keras.layers.LayerNormalization): The LayerNormalization layer.\n",
    "        add (tf.keras.layers.Add): The Add layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs: dict):\n",
    "        \"\"\"\n",
    "        Constructor of the BaseAttention layer.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Additional keyword arguments that are passed to the MultiHeadAttention layer, e.g.\n",
    "            num_heads (number of heads), key_dim (dimensionality of the key space), etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "    \"\"\"\n",
    "    A class that implements the cross-attention layer by inheriting from the BaseAttention class.\n",
    "    This layer is used to process two different sequences and attends to the context sequence while processing the query sequence.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    call: Performs the forward pass of the layer.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    mha (tf.keras.layers.MultiHeadAttention): The MultiHeadAttention layer.\n",
    "    layernorm (tf.keras.layers.LayerNormalization): The LayerNormalization layer.\n",
    "    add (tf.keras.layers.Add): The Add layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, x: tf.Tensor, context: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        The call function that performs the cross-attention operation.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        x (tf.Tensor): The query (expected Transformer results) sequence of shape (batch_size, seq_length, d_model).\n",
    "        context (tf.Tensor): The context (inputs to the Encoder layer) sequence of shape (batch_size, seq_length, d_model).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tf.Tensor: The output sequence of shape (batch_size, seq_length, d_model).\n",
    "        \"\"\"\n",
    "        attn_output, attn_scores = self.mha(query=x, key=context, value=context, return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
