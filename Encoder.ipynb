{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A single layer of the Encoder. Usually there are multiple layers stacked on top of each other.\n",
    "\n",
    "    Methods:\n",
    "        call: Performs the forward pass of the layer.\n",
    "\n",
    "    Attributes:\n",
    "        self_attention (GlobalSelfAttention): The global self-attention layer.\n",
    "        ffn (FeedForward): The feed-forward layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, dff: int, dropout_rate: float=0.1):\n",
    "        \"\"\"\n",
    "        Constructor of the EncoderLayer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimensionality of the model.\n",
    "            num_heads (int): The number of heads in the multi-head attention layer.\n",
    "            dff (int): The dimensionality of the feed-forward layer.\n",
    "            dropout_rate (float): The dropout rate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate\n",
    "            )\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        The call function that performs the forward pass of the layer.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): The input sequence of shape (batch_size, seq_length, d_model).\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output sequence of shape (batch_size, seq_length, d_model).\n",
    "        \"\"\"\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nx represents the count of how many EncoderLayers we use in the whole Encoder. Let's implement the Encoder layer in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A custom TensorFlow layer that implements the Encoder. This layer is mostly used in the Transformer models \n",
    "    for natural language processing tasks, such as machine translation, text summarization or text classification.\n",
    "\n",
    "    Methods:\n",
    "        call: Performs the forward pass of the layer.\n",
    "\n",
    "    Attributes:\n",
    "        d_model (int): The dimensionality of the model.\n",
    "        num_layers (int): The number of layers in the encoder.\n",
    "        pos_embedding (PositionalEmbedding): The positional embedding layer.\n",
    "        enc_layers (list): The list of encoder layers.\n",
    "        dropout (tf.keras.layers.Dropout): The dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, dff: int, vocab_size: int, dropout_rate: float=0.1):\n",
    "        \"\"\"\n",
    "        Constructor of the Encoder.\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): The number of layers in the encoder.\n",
    "            d_model (int): The dimensionality of the model.\n",
    "            num_heads (int): The number of heads in the multi-head attention layer.\n",
    "            dff (int): The dimensionality of the feed-forward layer.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "            dropout_rate (float): The dropout rate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                        num_heads=num_heads,\n",
    "                        dff=dff,\n",
    "                        dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        The call function that performs the forward pass of the layer.\n",
    "        \n",
    "        Args:\n",
    "            x (tf.Tensor): The input sequence of shape (batch_size, seq_length).\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output sequence of shape (batch_size, seq_length, d_model).\n",
    "        \"\"\"\n",
    "        x = self.pos_embedding(x)  \n",
    "        # here x has shape `(batch_size, seq_len, d_model)`\n",
    "\n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
